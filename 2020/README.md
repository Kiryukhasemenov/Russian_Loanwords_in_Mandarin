| Folder | Type        | Filename           | Description  |
|:-------------:|:-------------:|:-------------:|:-----:|
| [`phonetics`](../../blob/master/phonetics) | code (.ipynb) | [phonetics_study.ipynb](../../blob/master/phonetics/phonetics_study.ipynb) | Study of the phonetic adaptation of the Russian consonantal clusters, vowels and stress-to-tone transformations. Based on subset of Wikidata items filtered by BKRS and manually (`data_translit_cleared.csv`, `bkrs_epentheses.csv`), and from the Chinese Loanword Dictionary (`wlc_cd.csv`, `wlc_cd_epentheses.csv`, for details, see corresponding [README](../../blob/master/data) file) |
| [`morphemes`](../../blob/master/morphemes) | data (.xlsx) |[allomorphy_data.xlsx](../../blob/master/allomorphy_data.xlsx)   | Data used in study of the allomorphy adaptation of the Russian stems in Chinese. Based on Wikipedia dataset (input: `data_total.csv`) Chinese Loanword Dictionary (`wlc_cd.csv`) and the official transliteration guidelines (for details, see corresponding [README](../../blob/master/data) file) |
| [`CWS_algorithms`](../../blob/master/CWS_algorithms) | code (.ipynb) | [segmenters_comparison.ipynb](../../blob/master/CWS_algorithms/segmenters_comparison.ipynb) | Comparison of the Chinese word segmenters for the task of the Russian loanword detection. Algorithms used: wordlist extractor (see below); [jieba](https://github.com/fxsjy/jieba); [PKUSEG](https://github.com/lancopku/pkuseg-python); [Stanford CoreNLPTokenizer](https://stanfordnlp.github.io/CoreNLP/) (use this [manual](https://github.com/nltk/nltk/pull/1735) to install the working version). The data used for evaluation and analysis: `books_subset.csv`, `newspapers_subset.csv`, `books_subset_analyzed.csv`, `news_subset_analyzed.csv`. For details, see corresponding [README](../../blob/master/data) file | 
| [`CWS_algorithms`](../../blob/master/CWS_algorithms) | code (.py) | [wordlist_segmenter.py](../../blob/master/CWS_algorithms/wordlist_segmenter.py) | Manually created extractor from the [wordlist](../../blob/master/data/long_propers_list.txt). The list of words is taken from the official transliteration guidelines (for details, see corresponding [README](../../blob/master/data) file). The principle of the extractor is based on greedy search of the biggest matching string in the Chinese sentence. |
| current directory | document (.pdf) | Thesis text | The full text of the thesis (under preparation) |
