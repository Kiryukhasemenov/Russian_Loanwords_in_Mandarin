| Folder | Type        | Filename           | Description  |
|:-------------:|:-------------:|:-------------:|:-----:|
| current directory | document (.pdf) | [Russian Loanwords in Standard Chinese: Problems of Phonetic and Morpological Adaptation, and Automatic Recognition in Chinese Texts](https://www.hse.ru/en/edu/vkr/368892817) | Full text of the BA thesis presented in 2020 (abstract - English, text - Russian). |
| [`phonetics`](phonetics) | code (.ipynb) | [phonetics_study.ipynb](phonetics/phonetics_study.ipynb) | Study of the phonetic adaptation of the Russian consonantal clusters, vowels and stress-to-tone transformations. Based on subset of Wikidata items filtered by BKRS and manually (`data_translit_cleared.csv`, `bkrs_epentheses.csv`), and from the Chinese Loanword Dictionary (`wlc_cd.csv`, `wlc_cd_epentheses.csv`, for details, see corresponding [README](../data) file) |
| [`morphemes`](morphemes) | data (.xlsx) |[allomorphy_data.xlsx](morphemes/allomorphy_data.xlsx)   | Data used in study of the allomorphy adaptation of the Russian stems in Chinese. Based on Wikipedia dataset (input: `data_total.csv`) Chinese Loanword Dictionary (`wlc_cd.csv`) and the official transliteration guidelines (for details, see corresponding [README](../data) file) |
| [`CWS_algorithms`](CWS_algorithms) | code (.ipynb) | [segmenters_comparison.ipynb](CWS_algorithms/segmenters_comparison.ipynb) | Comparison of the Chinese word segmenters for the task of the Russian loanword detection. Algorithms used: wordlist extractor (see below); [jieba](https://github.com/fxsjy/jieba); [PKUSEG](https://github.com/lancopku/pkuseg-python); [Stanford CoreNLPTokenizer](https://stanfordnlp.github.io/CoreNLP/) (use this [manual](https://github.com/nltk/nltk/pull/1735) to install the working version). The data used for evaluation and analysis: `books_subset.csv`, `newspapers_subset.csv`, `books_subset_analyzed.csv`, `news_subset_analyzed.csv`. For details, see corresponding [README](../data) file | 
| [`CWS_algorithms`](CWS_algorithms) | code (.py) | [wordlist_segmenter.py](CWS_algorithms/wordlist_segmenter.py) | Manually created extractor from the [wordlist](../data/long_propers_list.txt). The list of words is taken from the official transliteration guidelines (for details, see corresponding [README](../data) file). The principle of the extractor is based on greedy search of the biggest matching string in the Chinese sentence. |
